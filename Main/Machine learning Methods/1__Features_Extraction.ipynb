{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction using HOG, ALBP, LLBP Methods\n",
    "### Author: BELHADDAD Mohamed Islem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Proprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# Preprocessing Methods\n",
    "\n",
    "def augmenter_image(image):\n",
    "    angle = random.randint(-10, 10)\n",
    "    height, width = image.shape[:2]\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1)\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
    "    return rotated_image\n",
    "\n",
    "\n",
    "def redimensionner_image(image, width, height):\n",
    "    return cv2.resize(image, (width, height))\n",
    "\n",
    "\n",
    "def is_image_file(file_path):\n",
    "    return file_path.lower().endswith('.png')\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140]).astype('int')\n",
    "\n",
    "# =====================================================================================\n",
    "# CSV_Extraction Method\n",
    "\n",
    "def save_image_to_csv(image, file_path):\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(image)\n",
    "\n",
    "def load_image_from_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        data = list(reader)\n",
    "        image = np.array(data).astype(np.uint8)\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Feature extraction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================================================\n",
    "# Feature Extraction\n",
    "\n",
    "class compute_hog():\n",
    "    def __init__(self, img, cell_size=16, bin_size=8):\n",
    "        self.img = img\n",
    "        self.img = np.sqrt(img / float(np.max(img)))\n",
    "        self.img = self.img * 255\n",
    "        self.cell_size = cell_size\n",
    "        self.bin_size = bin_size\n",
    "        self.angle_unit = 360 / self.bin_size\n",
    "        assert type(self.bin_size) == int, \"bin_size should be integer,\"\n",
    "        assert type(self.cell_size) == int, \"cell_size should be integer,\"\n",
    "        #assert type(self.angle_unit) == int, \"bin_size should be divisible by 360\"\n",
    "\n",
    "    def extract(self):\n",
    "        height, width = self.img.shape\n",
    "        gradient_magnitude, gradient_angle = self.global_gradient()\n",
    "        gradient_magnitude = abs(gradient_magnitude)\n",
    "        cell_gradient_vector = np.zeros((int(height / self.cell_size), int(width / self.cell_size), self.bin_size))\n",
    "        for i in range(cell_gradient_vector.shape[0]):\n",
    "            for j in range(cell_gradient_vector.shape[1]):\n",
    "                cell_magnitude = gradient_magnitude[i * self.cell_size:(i + 1) * self.cell_size,\n",
    "                                 j * self.cell_size:(j + 1) * self.cell_size]\n",
    "                cell_angle = gradient_angle[i * self.cell_size:(i + 1) * self.cell_size,\n",
    "                             j * self.cell_size:(j + 1) * self.cell_size]\n",
    "                cell_gradient_vector[i][j] = self.cell_gradient(cell_magnitude, cell_angle)\n",
    "\n",
    "        hog_image = self.render_gradient(np.zeros([height, width]), cell_gradient_vector)\n",
    "        hog_vector = []\n",
    "        for i in range(cell_gradient_vector.shape[0] - 1):\n",
    "            for j in range(cell_gradient_vector.shape[1] - 1):\n",
    "                block_vector = []\n",
    "                block_vector.extend(cell_gradient_vector[i][j])\n",
    "                block_vector.extend(cell_gradient_vector[i][j + 1])\n",
    "                block_vector.extend(cell_gradient_vector[i + 1][j])\n",
    "                block_vector.extend(cell_gradient_vector[i + 1][j + 1])\n",
    "                mag = lambda vector: math.sqrt(sum(i ** 2 for i in vector))\n",
    "                magnitude = mag(block_vector)\n",
    "                if magnitude != 0:\n",
    "                    normalize = lambda block_vector, magnitude: [element / magnitude for element in block_vector]\n",
    "                    block_vector = normalize(block_vector, magnitude)\n",
    "                hog_vector.append(block_vector)\n",
    "        return hog_vector, hog_image\n",
    "\n",
    "    def global_gradient(self):\n",
    "        gradient_values_x = cv2.Sobel(self.img, cv2.CV_64F, 1, 0, ksize=5)\n",
    "        gradient_values_y = cv2.Sobel(self.img, cv2.CV_64F, 0, 1, ksize=5)\n",
    "        gradient_magnitude = cv2.addWeighted(gradient_values_x, 0.5, gradient_values_y, 0.5, 0)\n",
    "        gradient_angle = cv2.phase(gradient_values_x, gradient_values_y, angleInDegrees=True)\n",
    "        return gradient_magnitude, gradient_angle\n",
    "\n",
    "    def cell_gradient(self, cell_magnitude, cell_angle):\n",
    "        orientation_centers = [0] * self.bin_size\n",
    "        for i in range(cell_magnitude.shape[0]):\n",
    "            for j in range(cell_magnitude.shape[1]):\n",
    "                gradient_strength = cell_magnitude[i][j]\n",
    "                gradient_angle = cell_angle[i][j]\n",
    "                min_angle, max_angle, mod = self.get_closest_bins(gradient_angle)\n",
    "                orientation_centers[min_angle] += (gradient_strength * (1 - (mod / self.angle_unit)))\n",
    "                orientation_centers[max_angle] += (gradient_strength * (mod / self.angle_unit))\n",
    "        return orientation_centers\n",
    "\n",
    "    def get_closest_bins(self, gradient_angle):\n",
    "        idx = int(gradient_angle / self.angle_unit)\n",
    "        mod = gradient_angle % self.angle_unit\n",
    "        if idx == self.bin_size:\n",
    "            return idx - 1, (idx) % self.bin_size, mod\n",
    "        return idx, (idx + 1) % self.bin_size, mod\n",
    "\n",
    "    def render_gradient(self, image, cell_gradient):\n",
    "        cell_width = self.cell_size / 2\n",
    "        max_mag = np.array(cell_gradient).max()\n",
    "        for x in range(cell_gradient.shape[0]):\n",
    "            for y in range(cell_gradient.shape[1]):\n",
    "                cell_grad = cell_gradient[x][y]\n",
    "                cell_grad /= max_mag\n",
    "                angle = 0\n",
    "                angle_gap = self.angle_unit\n",
    "                for magnitude in cell_grad:\n",
    "                    angle_radian = math.radians(angle)\n",
    "                    x1 = int(x * self.cell_size + magnitude * cell_width * math.cos(angle_radian))\n",
    "                    y1 = int(y * self.cell_size + magnitude * cell_width * math.sin(angle_radian))\n",
    "                    x2 = int(x * self.cell_size - magnitude * cell_width * math.cos(angle_radian))\n",
    "                    y2 = int(y * self.cell_size - magnitude * cell_width * math.sin(angle_radian))\n",
    "                    cv2.line(image, (y1, x1), (y2, x2), int(255 * math.sqrt(magnitude)))\n",
    "                    angle += angle_gap\n",
    "        return image\n",
    "\n",
    "def compute_llbp(image):\n",
    "    llbp_image = np.zeros_like(image, dtype=np.float32)\n",
    "    weights = np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=np.float32) / 16\n",
    "    for i in range(1, image.shape[0] - 1):\n",
    "        for j in range(1, image.shape[1] - 1):\n",
    "            value = 0\n",
    "            for m in range(-1, 2):\n",
    "                for n in range(-1, 2):\n",
    "                    value += image[i + m, j + n] * weights[m + 1, n + 1]\n",
    "            llbp_image[i, j] = np.clip(value, 0, 255)\n",
    "    return llbp_image\n",
    "\n",
    "def compute_albp(image):\n",
    "    albp_image = np.zeros_like(image)\n",
    "    for i in range(1, image.shape[0] - 1):\n",
    "        for j in range(1, image.shape[1] - 1):\n",
    "            center = image[i, j]\n",
    "            value = 0\n",
    "            for m in range(-1, 2):\n",
    "                for n in range(-1, 2):\n",
    "                    if m != 0 or n != 0:\n",
    "                        value += int(image[i + m, j + n] >= center) * 2**((3 * (m + 1)) + (n + 1))\n",
    "            albp_image[i, j] = np.clip(value, 0, 255)\n",
    "    return albp_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =====================================================================================\n",
    "# Application\n",
    "\n",
    "def process_image(image_path, output_folder):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Creation of LBP|LLBP|ALBP Folders \n",
    "    person_name = os.path.basename(os.path.dirname(image_path))     # Get the person id\n",
    "    # person_output_folder = os.path.join(output_folder, person_name) # Get the person folder\n",
    "    person_output_folder = output_folder\n",
    "    hog_folder = os.path.join(person_output_folder, 'HOG')\n",
    "    llbp_folder = os.path.join(person_output_folder, 'LLBP')\n",
    "    albp_folder = os.path.join(person_output_folder, 'ALBP')\n",
    "    os.makedirs(hog_folder, exist_ok=True)\n",
    "    os.makedirs(llbp_folder, exist_ok=True)\n",
    "    os.makedirs(albp_folder, exist_ok=True)\n",
    "\n",
    "    #  (Original Image) Extract and save features ======================================\n",
    "    # Convert to grayscale and resize\n",
    "    # Filtering\n",
    "    filtered_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    # Gray_Scaling\n",
    "    gray_image = cv2.cvtColor(filtered_image, cv2.COLOR_BGR2GRAY)\n",
    "    # Resizing\n",
    "    width, height = 100, 150\n",
    "    resized_image = redimensionner_image(gray_image, width, height)\n",
    "    # Compute features\n",
    "    # lbp_image = compute_hog(resized_image)\n",
    "    hog = compute_hog(resized_image, cell_size=6, bin_size=5)\n",
    "    vector, hog_image = hog.extract()\n",
    "    llbp_image = compute_llbp(resized_image)\n",
    "    albp_image = compute_albp(resized_image)\n",
    "    # Save the features to CSV\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    hog_image_path = os.path.join(hog_folder, f'hog_features_{base_name}.csv')\n",
    "    llbp_image_path = os.path.join(llbp_folder, f'llbp_features_{base_name}.csv')\n",
    "    albp_image_path = os.path.join(albp_folder, f'albp_features_{base_name}.csv')\n",
    "    save_image_to_csv(hog_image, hog_image_path)\n",
    "    save_image_to_csv(llbp_image, llbp_image_path)\n",
    "    save_image_to_csv(albp_image, albp_image_path)\n",
    "    # ==================================================================================\n",
    "\n",
    "\n",
    "    augmented_images = []\n",
    "    for _ in range(3):  # 2 augmentations par image\n",
    "        augmented_image = augmenter_image(image)\n",
    "        augmented_images.append(augmented_image)\n",
    "\n",
    "    for i, augmented_image in enumerate(augmented_images, start=1):\n",
    "\n",
    "        # Filtering\n",
    "        filtered_image = cv2.GaussianBlur(augmented_image, (5, 5), 0)\n",
    "        # Gray_Scaling\n",
    "        gray_image = cv2.cvtColor(filtered_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Resizing\n",
    "        width, height = 100, 150\n",
    "        resized_image = redimensionner_image(gray_image, width, height)\n",
    "\n",
    "        # Compute LBP|ALBP|LLBP (Features Extraction)\n",
    "        # lbp_img = compute_hog(resized_image)\n",
    "        hog = compute_hog(resized_image, cell_size=6, bin_size=5)\n",
    "        vector, hog_image = hog.extract()\n",
    "        llbp_img = compute_llbp(resized_image)\n",
    "        albp_img = compute_albp(resized_image)\n",
    "\n",
    "        # Save LBP|ALBP|LLBP image to CSV\n",
    "        hog_image_path = hog_folder + f'/hog_features_{os.path.splitext(os.path.basename(image_path))[0]}_{i}.csv'\n",
    "        llbp_image_path = llbp_folder + f'/llbp_features_{os.path.splitext(os.path.basename(image_path))[0]}_{i}.csv'\n",
    "        albp_image_path = albp_folder + f'/albp_features_{os.path.splitext(os.path.basename(image_path))[0]}_{i}.csv'\n",
    "        save_image_to_csv(hog_image, hog_image_path)\n",
    "        save_image_to_csv(llbp_img, llbp_image_path)\n",
    "        save_image_to_csv(albp_img, albp_image_path)\n",
    "        # print(lbp_image_path)\n",
    "        # print(llbp_image_path)\n",
    "        # print(albp_image_path)\n",
    "\n",
    "\n",
    "def preprocess_images(database_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for person_folder in os.listdir(database_folder):\n",
    "        person_folder_path = os.path.join(database_folder, person_folder)   # data/AMI/X\n",
    "        print(person_folder_path)\n",
    "        if os.path.isdir(person_folder_path):\n",
    "            person_output_folder = os.path.join(output_folder, person_folder)   # output_folder/X\n",
    "            os.makedirs(person_output_folder, exist_ok=True)\n",
    "\n",
    "            for image_file in os.listdir(person_folder_path):\n",
    "                if is_image_file(image_file):\n",
    "                    image_path = os.path.join(person_folder_path, image_file)   # data/AMI/X/img.png\n",
    "                    process_image(image_path, person_output_folder)\n",
    "\n",
    "database_folder = 'C:/Users/PC-MOH/Desktop/Rahmani Deep leanring/AMI'\n",
    "preprocessed_folder = 'data/extract_features'\n",
    "# lbp_folder = 'data/lbp_features'\n",
    "\n",
    "preprocess_images(database_folder, preprocessed_folder)\n",
    "# preprocess_images(preprocessed_folder, lbp_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Divide and Combine dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Combine samples in one csv file\n",
    "Note: \"This code is deals with ALBP method... change the code based on your specific method\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Directory where all class folders are stored\n",
    "root_directory = \"data\\extract_features\"  # Update with your directory path\n",
    "\n",
    "# List of all folder names (each folder represents a class)\n",
    "folder_names = [f.name for f in os.scandir(root_directory) if f.is_dir()]\n",
    "\n",
    "# List to hold all DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through each folder to get CSV files and their label from the folder name\n",
    "for folder_name in folder_names:\n",
    "    folder_path = os.path.join(root_directory, folder_name) # Person_path\n",
    "    # Get all CSV files in this folder\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"ALBP\", \"*.csv\"))\n",
    "    print('person: '+str(folder_name))\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # Read the CSV file, without headers (assuming no headers in original CSV)\n",
    "        df = pd.read_csv(csv_file, header=None)  # No header\n",
    "\n",
    "        # Flatten the DataFrame to 1D\n",
    "        flattened_array = df.values.flatten()  # Convert DataFrame to a 1D array\n",
    "\n",
    "        # Convert the flattened array back to a DataFrame with a single row\n",
    "        flattened_df = pd.DataFrame([flattened_array])\n",
    "\n",
    "        # Save to a new CSV file (with or without header as desired)\n",
    "        # flattened_df.to_csv(\"flattened_output.csv\", index=False, header=False)  # No header, single row\n",
    "        \n",
    "        # Add a new column for the label based on folder name\n",
    "        flattened_df[\"label\"] = folder_name\n",
    "        \n",
    "        # Append this DataFrame to the list\n",
    "        dataframes.append(flattened_df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(combined_df.shape)\n",
    "print(f\"Image files successfully flattened\")\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file without a header\n",
    "# output_csv_path = \"data/combined_data_ALBP.csv\"  # Output CSV path\n",
    "# combined_df.to_csv(output_csv_path, index=False)  # Write without headers\n",
    "# df = combined_df\n",
    "# df = pd.read_csv(output_csv_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train/Test Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the entire CSV\n",
    "# csv_path = \"data\\combined_data_ALBP.csv\" \n",
    "# data = pd.read_csv(csv_path)  \n",
    "print(\"Divsier Train/Test...\")\n",
    "data = combined_df\n",
    "\n",
    "# Number of rows to slice for training and testing\n",
    "train_size = 25\n",
    "test_size = 3\n",
    "iteration_size = train_size + test_size\n",
    "\n",
    "# Total number of iterations\n",
    "total_rows = data.shape[0]\n",
    "iterations = total_rows // iteration_size  # Full iterations available in the dataset\n",
    "\n",
    "# List to store the results of training and testing\n",
    "train_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "# Iterate through the data to create training and testing sets\n",
    "for i in range(iterations):\n",
    "    print('iteration: ', i)\n",
    "    start = i * iteration_size\n",
    "    end = start + iteration_size\n",
    "    \n",
    "    # Slice out the train and test sets for this iteration\n",
    "    train_df = data.iloc[start:start + train_size]\n",
    "    test_df = data.iloc[start + train_size:start + iteration_size]\n",
    "    \n",
    "    # Store the results\n",
    "    train_dfs.append(train_df)\n",
    "    test_dfs.append(test_df)\n",
    "\n",
    "# Concatenate all training sets and all testing sets\n",
    "final_train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "final_test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "\n",
    "# Save the final training and testing datasets\n",
    "final_train_df.to_csv(\"data/train_test/ALBP/train_ALBP.csv\", index=False) \n",
    "final_test_df.to_csv(\"data/train_test/ALBP/test_ALBP.csv\", index=False)   \n",
    "\n",
    "\n",
    "print(\"Training and testing datasets have been created and saved\")\n",
    "print(\"Training Shape:\" + str(final_train_df.shape))\n",
    "print(\"Testing Shape:\" + str(final_test_df.shape))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
